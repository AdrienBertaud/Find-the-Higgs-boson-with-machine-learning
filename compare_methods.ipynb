{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from implementations import *\n",
    "from evaluation import *\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "# Standardize data\n",
    "tX, tX_test = standardize(tX, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 30)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 85667, -1.0: 164333})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 30)\n",
      "(50000, 30)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = split_data(tX, y, 0.8)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "evaluation_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-76b18c50203a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mevaluation_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mevaluation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/university/MachineLearningProject/evaluation.py\u001b[0m in \u001b[0;36mcross_val\u001b[0;34m(tX, y, splits, method, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mloss_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mcv_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "#max_iters = [10, 100, 500, 750, 1000, 1500]\n",
    "#gammas = [0.01, 0.049, 0.05, 0.053, 0.07, 0.1]\n",
    "max_iters = [100]\n",
    "gammas = [0.1]\n",
    "\n",
    "w = 0\n",
    "\n",
    "for max_iter in max_iters:\n",
    "    for gamma in gammas:\n",
    "        evaluation_result = cross_val(tX, y, 10, least_squares_GD, initial_w=w_initial, max_iters=max_iter, gamma=gamma)\n",
    "        \n",
    "        evaluation_data.append(evaluation_result)\n",
    "        print(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Export Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'results\\least_squares_GD.csv'\n",
    "y_pred = predict_labels(w, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'least_squares_SGD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '4.211122', 'train_loss': '77.66278943722936', 'test_loss': '77.87536604619282', 'accuracy': '0.50516', 'f1_score': '0.3945874522854067', 'precision': '0.341103308232507', 'recall': '0.46796285548461986', 'confusion_matrix': {'tp': 8063, 'fp': 15575, 'tn': 17195, 'fn': 9167}}\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "#max_iters = [10, 100, 500, 750, 1000, 1500]\n",
    "#gammas = [0.01, 0.049, 0.05, 0.053, 0.07, 0.1]\n",
    "max_iters = [100]\n",
    "gammas = [0.1]\n",
    "\n",
    "for max_iter in max_iters:\n",
    "    for gamma in gammas:\n",
    "        # execute training and loss calculation\n",
    "        start_time = datetime.datetime.now()\n",
    "        w_initial = np.array(np.zeros(30))\n",
    "        w, loss_train = least_squares_SGD(y_train, x_train, w_initial, max_iter, gamma)\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "        end_time = datetime.datetime.now()\n",
    "        \n",
    "        # create dict with results for run\n",
    "        evaluation_result = {}\n",
    "        evaluation_result['method'] = 'least_squares_SGD'\n",
    "        evaluation_result['parameters'] = {}\n",
    "        evaluation_result['parameters']['max_iter'] = max_iter\n",
    "        evaluation_result['parameters']['gamma'] = gamma\n",
    "        evaluation_result['training_and_loss_calc_time_in_sec'] = str((end_time - start_time).total_seconds())\n",
    "        evaluation_result['train_loss'] = str(loss_train)\n",
    "        evaluation_result['test_loss'] = str(loss_test)\n",
    "        evaluation_result['accuracy'] = str(calculate_accuracy(w, x_test, y_test))\n",
    "        f1_score, precision, recall, tp, fp, tn, fn = calculate_f1_score(w, x_test, y_test)\n",
    "        evaluation_result['f1_score'] = str(f1_score)\n",
    "        evaluation_result['precision'] = str(precision)\n",
    "        evaluation_result['recall'] = str(recall)\n",
    "        evaluation_result['confusion_matrix'] = {}\n",
    "        evaluation_result['confusion_matrix']['tp'] = tp\n",
    "        evaluation_result['confusion_matrix']['fp'] = fp\n",
    "        evaluation_result['confusion_matrix']['tn'] = tn\n",
    "        evaluation_result['confusion_matrix']['fn'] = fn\n",
    "        \n",
    "        evaluation_data.append(evaluation_result)\n",
    "        print(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'least_squares', 'parameters': {}, 'training_and_loss_calc_time_in_sec': '0.024671', 'train_loss': '0.38885794352313174', 'test_loss': '0.38936695348103784', 'accuracy': '0.71676', 'f1_score': '0.6650425733207189', 'precision': '0.5612375249500998', 'recall': '0.8159605339524086', 'confusion_matrix': {'tp': 14059, 'fp': 10991, 'tn': 21779, 'fn': 3171}}\n"
     ]
    }
   ],
   "source": [
    "# execute training and loss calculation\n",
    "start_time = datetime.datetime.now()\n",
    "w_initial = np.array(np.zeros(30))\n",
    "w, loss_train = least_squares(y_train, x_train)\n",
    "loss_test = compute_loss(y_test, x_test, w)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# create dict with results for run\n",
    "evaluation_result = {}\n",
    "evaluation_result['method'] = 'least_squares'\n",
    "evaluation_result['parameters'] = {}\n",
    "evaluation_result['training_and_loss_calc_time_in_sec'] = str((end_time - start_time).total_seconds())\n",
    "evaluation_result['train_loss'] = str(loss_train)\n",
    "evaluation_result['test_loss'] = str(loss_test)\n",
    "evaluation_result['accuracy'] = str(calculate_accuracy(w, x_test, y_test))\n",
    "f1_score, precision, recall, tp, fp, tn, fn = calculate_f1_score(w, x_test, y_test)\n",
    "evaluation_result['f1_score'] = str(f1_score)\n",
    "evaluation_result['precision'] = str(precision)\n",
    "evaluation_result['recall'] = str(recall)\n",
    "evaluation_result['confusion_matrix'] = {}\n",
    "evaluation_result['confusion_matrix']['tp'] = tp\n",
    "evaluation_result['confusion_matrix']['fp'] = fp\n",
    "evaluation_result['confusion_matrix']['tn'] = tn\n",
    "evaluation_result['confusion_matrix']['fn'] = fn\n",
    "\n",
    "evaluation_data.append(evaluation_result)\n",
    "print(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'ridge_regression', 'parameters': {'lambda': 100}, 'training_and_loss_calc_time_in_sec': '0.030448', 'train_loss': '0.4970710829640738', 'test_loss': '0.4970506235215197', 'accuracy': '0.63862', 'f1_score': '0.5369891095451634', 'precision': '0.48075246616196377', 'recall': '0.608125362739408', 'confusion_matrix': {'tp': 10478, 'fp': 11317, 'tn': 21453, 'fn': 6752}}\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "#lambda = [10, 100, 500, 750, 1000, 1500]\n",
    "lambdas = [100]\n",
    "\n",
    "for alambda in lambdas:\n",
    "    # execute training and loss calculation\n",
    "    start_time = datetime.datetime.now()\n",
    "    w_initial = np.array(np.zeros(30))\n",
    "    w, loss_train = ridge_regression(y_train, x_train, alambda)\n",
    "    loss_test = compute_loss(y_test, x_test, w)\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    # create dict with results for run\n",
    "    evaluation_result = {}\n",
    "    evaluation_result['method'] = 'ridge_regression'\n",
    "    evaluation_result['parameters'] = {}\n",
    "    evaluation_result['parameters']['lambda'] = alambda\n",
    "    evaluation_result['training_and_loss_calc_time_in_sec'] = str((end_time - start_time).total_seconds())\n",
    "    evaluation_result['train_loss'] = str(loss_train)\n",
    "    evaluation_result['test_loss'] = str(loss_test)\n",
    "    evaluation_result['accuracy'] = str(calculate_accuracy(w, x_test, y_test))\n",
    "    f1_score, precision, recall, tp, fp, tn, fn = calculate_f1_score(w, x_test, y_test)\n",
    "    evaluation_result['f1_score'] = str(f1_score)\n",
    "    evaluation_result['precision'] = str(precision)\n",
    "    evaluation_result['recall'] = str(recall)\n",
    "    evaluation_result['confusion_matrix'] = {}\n",
    "    evaluation_result['confusion_matrix']['tp'] = tp\n",
    "    evaluation_result['confusion_matrix']['fp'] = fp\n",
    "    evaluation_result['confusion_matrix']['tn'] = tn\n",
    "    evaluation_result['confusion_matrix']['fn'] = fn\n",
    "\n",
    "    evaluation_data.append(evaluation_result)\n",
    "    print(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration = 0, loss=1496504.0880804332\n",
      "Current iteration = 50, loss=1614141.5617615525\n",
      "{'method': 'logistic_regression', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '9.335937', 'train_loss': '1312169.7718320512', 'test_loss': '678243337.0656612', 'accuracy': '0.68286', 'f1_score': '0.5886960807200476', 'precision': '0.5321952820897622', 'recall': '0.6586186883343006', 'confusion_matrix': {'tp': 11348, 'fp': 9975, 'tn': 22795, 'fn': 5882}}\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "#max_iters = [10, 100, 500, 750, 1000, 1500]\n",
    "#gammas = [0.01, 0.049, 0.05, 0.053, 0.07, 0.1]\n",
    "max_iters = [100]\n",
    "gammas = [0.1]\n",
    "\n",
    "for max_iter in max_iters:\n",
    "    for gamma in gammas:\n",
    "        # execute training and loss calculation\n",
    "        start_time = datetime.datetime.now()\n",
    "        w_initial = np.array(np.zeros(30))\n",
    "        w, loss_train = logistic_regression(y_train, x_train, w_initial, max_iter, gamma)\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "        end_time = datetime.datetime.now()\n",
    "        \n",
    "        # create dict with results for run\n",
    "        evaluation_result = {}\n",
    "        evaluation_result['method'] = 'logistic_regression'\n",
    "        evaluation_result['parameters'] = {}\n",
    "        evaluation_result['parameters']['max_iter'] = max_iter\n",
    "        evaluation_result['parameters']['gamma'] = gamma\n",
    "        evaluation_result['training_and_loss_calc_time_in_sec'] = str((end_time - start_time).total_seconds())\n",
    "        evaluation_result['train_loss'] = str(loss_train)\n",
    "        evaluation_result['test_loss'] = str(loss_test)\n",
    "        evaluation_result['accuracy'] = str(calculate_accuracy(w, x_test, y_test))\n",
    "        f1_score, precision, recall, tp, fp, tn, fn = calculate_f1_score(w, x_test, y_test)\n",
    "        evaluation_result['f1_score'] = str(f1_score)\n",
    "        evaluation_result['precision'] = str(precision)\n",
    "        evaluation_result['recall'] = str(recall)\n",
    "        evaluation_result['confusion_matrix'] = {}\n",
    "        evaluation_result['confusion_matrix']['tp'] = tp\n",
    "        evaluation_result['confusion_matrix']['fp'] = fp\n",
    "        evaluation_result['confusion_matrix']['tn'] = tn\n",
    "        evaluation_result['confusion_matrix']['fn'] = fn\n",
    "        \n",
    "        evaluation_data.append(evaluation_result)\n",
    "        print(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.759231', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866'}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.05}, 'training_and_loss_calc_time_in_sec': '0.713638', 'train_loss': '0.3987838539784226', 'test_loss': '0.39948206088489485', 'accuracy': '0.70718', 'f1_score': '0.6579764991706964', 'precision': '0.550611877859014', 'recall': '0.8173534532791642'}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 300, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '2.105392', 'train_loss': '0.3894785858650492', 'test_loss': '0.39012322881498535', 'accuracy': '0.71586', 'f1_score': '0.6649371477087804', 'precision': '0.5600492630408009', 'recall': '0.818165989553105'}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 300, 'gamma': 0.05}, 'training_and_loss_calc_time_in_sec': '2.093798', 'train_loss': '0.3905826615558456', 'test_loss': '0.3912308685795799', 'accuracy': '0.71422', 'f1_score': '0.6641280586700515', 'precision': '0.5580926796507724', 'recall': '0.8199071387115496'}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.764704', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866'}\n",
      "{'method': 'least_squares_SGD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '4.325422', 'train_loss': '1.782194603218081', 'test_loss': '1.7763050527157862', 'accuracy': '0.54988', 'f1_score': '0.48541247484909456', 'precision': '0.4004753640685128', 'recall': '0.6160766105629716'}\n",
      "{'method': 'ridge_regression', 'parameters': {'lambda': 100}, 'training_and_loss_calc_time_in_sec': '0.040133', 'train_loss': '0.4970710829640738', 'test_loss': '0.4970506235215197', 'accuracy': '0.63862', 'f1_score': '0.5369891095451634', 'precision': '0.48075246616196377', 'recall': '0.608125362739408'}\n",
      "{'method': 'least_squares', 'parameters': {}, 'training_and_loss_calc_time_in_sec': '0.023288', 'train_loss': '0.38885794352313174', 'test_loss': '0.38936695348103784', 'accuracy': '0.71676', 'f1_score': '0.6650425733207189', 'precision': '0.5612375249500998', 'recall': '0.8159605339524086'}\n",
      "{'method': 'logistic_regression', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '14.96455', 'train_loss': '-581240.0403015733', 'test_loss': '6886392392.59792', 'accuracy': '0.71078', 'f1_score': '0.6624337636266019', 'precision': '0.5540630247178726', 'recall': '0.8235055136390017'}\n",
      "{'method': 'logistic_regression', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '1.315524', 'train_loss': 'nan', 'test_loss': '6886392392.59792', 'accuracy': '0.71078', 'f1_score': '0.6624337636266019', 'precision': '0.5540630247178726', 'recall': '0.8235055136390017'}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.787177', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866', 'confusion_matrix': {'tp': 14125, 'fp': 11291, 'tn': 21479, 'fn': 3105}}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.780654', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866', 'confusion_matrix': {'tp': 14125, 'fp': 11291, 'tn': 21479, 'fn': 3105}}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.887559', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866', 'confusion_matrix': {'tp': 14125, 'fp': 11291, 'tn': 21479, 'fn': 3105}}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.809152', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866', 'confusion_matrix': {'tp': 14125, 'fp': 11291, 'tn': 21479, 'fn': 3105}}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.787098', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866', 'confusion_matrix': {'tp': 14125, 'fp': 11291, 'tn': 21479, 'fn': 3105}}\n",
      "{'method': 'least_squares_SGD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '4.211122', 'train_loss': '77.66278943722936', 'test_loss': '77.87536604619282', 'accuracy': '0.50516', 'f1_score': '0.3945874522854067', 'precision': '0.341103308232507', 'recall': '0.46796285548461986', 'confusion_matrix': {'tp': 8063, 'fp': 15575, 'tn': 17195, 'fn': 9167}}\n",
      "{'method': 'least_squares', 'parameters': {}, 'training_and_loss_calc_time_in_sec': '0.024671', 'train_loss': '0.38885794352313174', 'test_loss': '0.38936695348103784', 'accuracy': '0.71676', 'f1_score': '0.6650425733207189', 'precision': '0.5612375249500998', 'recall': '0.8159605339524086', 'confusion_matrix': {'tp': 14059, 'fp': 10991, 'tn': 21779, 'fn': 3171}}\n",
      "{'method': 'ridge_regression', 'parameters': {'lambda': 100}, 'training_and_loss_calc_time_in_sec': '0.030448', 'train_loss': '0.4970710829640738', 'test_loss': '0.4970506235215197', 'accuracy': '0.63862', 'f1_score': '0.5369891095451634', 'precision': '0.48075246616196377', 'recall': '0.608125362739408', 'confusion_matrix': {'tp': 10478, 'fp': 11317, 'tn': 21453, 'fn': 6752}}\n"
     ]
    }
   ],
   "source": [
    "for el in evaluation_data:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

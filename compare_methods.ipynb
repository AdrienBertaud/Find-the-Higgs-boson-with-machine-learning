{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from implementations import *\n",
    "from evaluation import *\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "# Standardize data\n",
    "tX = standardize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 85667, -1.0: 164333})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 30)\n",
      "(50000, 30)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = split_data(tX, y, 0.8)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "evaluation_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.787098', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866', 'confusion_matrix': {'tp': 14125, 'fp': 11291, 'tn': 21479, 'fn': 3105}}\n",
      "\n",
      "Confusion Matrix\n",
      "14125 11291\n",
      "21479 3105\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "#max_iters = [10, 100, 500, 750, 1000, 1500]\n",
    "#gammas = [0.01, 0.049, 0.05, 0.053, 0.07, 0.1]\n",
    "max_iters = [100]\n",
    "gammas = [0.1]\n",
    "\n",
    "\n",
    "for max_iter in max_iters:\n",
    "    for gamma in gammas:\n",
    "        # execute training and loss calculation\n",
    "        start_time = datetime.datetime.now()\n",
    "        w_initial = np.array(np.zeros(30))\n",
    "        w, loss_train = least_squares_GD(y_train, x_train, w_initial, max_iter, gamma)\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "        end_time = datetime.datetime.now()\n",
    "        \n",
    "        # create dict with results for run\n",
    "        evaluation_result = {}\n",
    "        evaluation_result['method'] = 'least_squares_GD'\n",
    "        evaluation_result['parameters'] = {}\n",
    "        evaluation_result['parameters']['max_iter'] = max_iter\n",
    "        evaluation_result['parameters']['gamma'] = gamma\n",
    "        evaluation_result['training_and_loss_calc_time_in_sec'] = str((end_time - start_time).total_seconds())\n",
    "        evaluation_result['train_loss'] = str(loss_train)\n",
    "        evaluation_result['test_loss'] = str(loss_test)\n",
    "        evaluation_result['accuracy'] = str(calculate_accuracy(w, x_test, y_test))\n",
    "        f1_score, precision, recall, tp, fp, tn, fn = calculate_f1_score(w, x_test, y_test)\n",
    "        evaluation_result['f1_score'] = str(f1_score)\n",
    "        evaluation_result['precision'] = str(precision)\n",
    "        evaluation_result['recall'] = str(recall)\n",
    "        evaluation_result['confusion_matrix'] = {}\n",
    "        evaluation_result['confusion_matrix']['tp'] = tp\n",
    "        evaluation_result['confusion_matrix']['fp'] = fp\n",
    "        evaluation_result['confusion_matrix']['tn'] = tn\n",
    "        evaluation_result['confusion_matrix']['fn'] = fn\n",
    "        \n",
    "        evaluation_data.append(evaluation_result)\n",
    "        print(evaluation_result)\n",
    "        \n",
    "        print()\n",
    "        print('Confusion Matrix')\n",
    "        print(str(evaluation_result['confusion_matrix']['tp']) + ' ' + str(evaluation_result['confusion_matrix']['fp']))\n",
    "        print(str(evaluation_result['confusion_matrix']['tn']) + ' ' + str(evaluation_result['confusion_matrix']['fn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'least_squares_SGD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '4.211122', 'train_loss': '77.66278943722936', 'test_loss': '77.87536604619282', 'accuracy': '0.50516', 'f1_score': '0.3945874522854067', 'precision': '0.341103308232507', 'recall': '0.46796285548461986', 'confusion_matrix': {'tp': 8063, 'fp': 15575, 'tn': 17195, 'fn': 9167}}\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "#max_iters = [10, 100, 500, 750, 1000, 1500]\n",
    "#gammas = [0.01, 0.049, 0.05, 0.053, 0.07, 0.1]\n",
    "max_iters = [100]\n",
    "gammas = [0.1]\n",
    "\n",
    "for max_iter in max_iters:\n",
    "    for gamma in gammas:\n",
    "        # execute training and loss calculation\n",
    "        start_time = datetime.datetime.now()\n",
    "        w_initial = np.array(np.zeros(30))\n",
    "        w, loss_train = least_squares_SGD(y_train, x_train, w_initial, max_iter, gamma)\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "        end_time = datetime.datetime.now()\n",
    "        \n",
    "        # create dict with results for run\n",
    "        evaluation_result = {}\n",
    "        evaluation_result['method'] = 'least_squares_SGD'\n",
    "        evaluation_result['parameters'] = {}\n",
    "        evaluation_result['parameters']['max_iter'] = max_iter\n",
    "        evaluation_result['parameters']['gamma'] = gamma\n",
    "        evaluation_result['training_and_loss_calc_time_in_sec'] = str((end_time - start_time).total_seconds())\n",
    "        evaluation_result['train_loss'] = str(loss_train)\n",
    "        evaluation_result['test_loss'] = str(loss_test)\n",
    "        evaluation_result['accuracy'] = str(calculate_accuracy(w, x_test, y_test))\n",
    "        f1_score, precision, recall, tp, fp, tn, fn = calculate_f1_score(w, x_test, y_test)\n",
    "        evaluation_result['f1_score'] = str(f1_score)\n",
    "        evaluation_result['precision'] = str(precision)\n",
    "        evaluation_result['recall'] = str(recall)\n",
    "        evaluation_result['confusion_matrix'] = {}\n",
    "        evaluation_result['confusion_matrix']['tp'] = tp\n",
    "        evaluation_result['confusion_matrix']['fp'] = fp\n",
    "        evaluation_result['confusion_matrix']['tn'] = tn\n",
    "        evaluation_result['confusion_matrix']['fn'] = fn\n",
    "        \n",
    "        evaluation_data.append(evaluation_result)\n",
    "        print(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'least_squares', 'parameters': {}, 'training_and_loss_calc_time_in_sec': '0.024671', 'train_loss': '0.38885794352313174', 'test_loss': '0.38936695348103784', 'accuracy': '0.71676', 'f1_score': '0.6650425733207189', 'precision': '0.5612375249500998', 'recall': '0.8159605339524086', 'confusion_matrix': {'tp': 14059, 'fp': 10991, 'tn': 21779, 'fn': 3171}}\n"
     ]
    }
   ],
   "source": [
    "# execute training and loss calculation\n",
    "start_time = datetime.datetime.now()\n",
    "w_initial = np.array(np.zeros(30))\n",
    "w, loss_train = least_squares(y_train, x_train)\n",
    "loss_test = compute_loss(y_test, x_test, w)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# create dict with results for run\n",
    "evaluation_result = {}\n",
    "evaluation_result['method'] = 'least_squares'\n",
    "evaluation_result['parameters'] = {}\n",
    "evaluation_result['training_and_loss_calc_time_in_sec'] = str((end_time - start_time).total_seconds())\n",
    "evaluation_result['train_loss'] = str(loss_train)\n",
    "evaluation_result['test_loss'] = str(loss_test)\n",
    "evaluation_result['accuracy'] = str(calculate_accuracy(w, x_test, y_test))\n",
    "f1_score, precision, recall, tp, fp, tn, fn = calculate_f1_score(w, x_test, y_test)\n",
    "evaluation_result['f1_score'] = str(f1_score)\n",
    "evaluation_result['precision'] = str(precision)\n",
    "evaluation_result['recall'] = str(recall)\n",
    "evaluation_result['confusion_matrix'] = {}\n",
    "evaluation_result['confusion_matrix']['tp'] = tp\n",
    "evaluation_result['confusion_matrix']['fp'] = fp\n",
    "evaluation_result['confusion_matrix']['tn'] = tn\n",
    "evaluation_result['confusion_matrix']['fn'] = fn\n",
    "\n",
    "evaluation_data.append(evaluation_result)\n",
    "print(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'ridge_regression', 'parameters': {'lambda': 100}, 'training_and_loss_calc_time_in_sec': '0.030448', 'train_loss': '0.4970710829640738', 'test_loss': '0.4970506235215197', 'accuracy': '0.63862', 'f1_score': '0.5369891095451634', 'precision': '0.48075246616196377', 'recall': '0.608125362739408', 'confusion_matrix': {'tp': 10478, 'fp': 11317, 'tn': 21453, 'fn': 6752}}\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "#lambda = [10, 100, 500, 750, 1000, 1500]\n",
    "lambdas = [100]\n",
    "\n",
    "for alambda in lambdas:\n",
    "    # execute training and loss calculation\n",
    "    start_time = datetime.datetime.now()\n",
    "    w_initial = np.array(np.zeros(30))\n",
    "    w, loss_train = ridge_regression(y_train, x_train, alambda)\n",
    "    loss_test = compute_loss(y_test, x_test, w)\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    # create dict with results for run\n",
    "    evaluation_result = {}\n",
    "    evaluation_result['method'] = 'ridge_regression'\n",
    "    evaluation_result['parameters'] = {}\n",
    "    evaluation_result['parameters']['lambda'] = alambda\n",
    "    evaluation_result['training_and_loss_calc_time_in_sec'] = str((end_time - start_time).total_seconds())\n",
    "    evaluation_result['train_loss'] = str(loss_train)\n",
    "    evaluation_result['test_loss'] = str(loss_test)\n",
    "    evaluation_result['accuracy'] = str(calculate_accuracy(w, x_test, y_test))\n",
    "    f1_score, precision, recall, tp, fp, tn, fn = calculate_f1_score(w, x_test, y_test)\n",
    "    evaluation_result['f1_score'] = str(f1_score)\n",
    "    evaluation_result['precision'] = str(precision)\n",
    "    evaluation_result['recall'] = str(recall)\n",
    "    evaluation_result['confusion_matrix'] = {}\n",
    "    evaluation_result['confusion_matrix']['tp'] = tp\n",
    "    evaluation_result['confusion_matrix']['fp'] = fp\n",
    "    evaluation_result['confusion_matrix']['tn'] = tn\n",
    "    evaluation_result['confusion_matrix']['fn'] = fn\n",
    "\n",
    "    evaluation_data.append(evaluation_result)\n",
    "    print(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logistic regression using gradient descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximilian/university/MachineLearningProject/implementations.py:73: RuntimeWarning: overflow encountered in exp\n",
      "  def ridge_regression(y, tx, lambda_):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'logistic_regression', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '1.315524', 'train_loss': 'nan', 'test_loss': '6886392392.59792', 'accuracy': '0.71078', 'f1_score': '0.6624337636266019', 'precision': '0.5540630247178726', 'recall': '0.8235055136390017'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximilian/university/MachineLearningProject/implementations.py:81: RuntimeWarning: divide by zero encountered in log\n",
      "  print(\"max = \", max(t))\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "#max_iters = [10, 100, 500, 750, 1000, 1500]\n",
    "#gammas = [0.01, 0.049, 0.05, 0.053, 0.07, 0.1]\n",
    "max_iters = [100]\n",
    "gammas = [0.1]\n",
    "\n",
    "for max_iter in max_iters:\n",
    "    for gamma in gammas:\n",
    "        # execute training and loss calculation\n",
    "        start_time = datetime.datetime.now()\n",
    "        w_initial = np.array(np.zeros(30))\n",
    "        w, loss_train = logistic_regression(y_train, x_train, w_initial, max_iter, gamma)\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "        end_time = datetime.datetime.now()\n",
    "        \n",
    "        # create dict with results for run\n",
    "        evaluation_result = {}\n",
    "        evaluation_result['method'] = 'logistic_regression'\n",
    "        evaluation_result['parameters'] = {}\n",
    "        evaluation_result['parameters']['max_iter'] = max_iter\n",
    "        evaluation_result['parameters']['gamma'] = gamma\n",
    "        evaluation_result['training_and_loss_calc_time_in_sec'] = str((end_time - start_time).total_seconds())\n",
    "        evaluation_result['train_loss'] = str(loss_train)\n",
    "        evaluation_result['test_loss'] = str(loss_test)\n",
    "        evaluation_result['accuracy'] = str(calculate_accuracy(w, x_test, y_test))\n",
    "        f1_score, precision, recall, tp, fp, tn, fn = calculate_f1_score(w, x_test, y_test)\n",
    "        evaluation_result['f1_score'] = str(f1_score)\n",
    "        evaluation_result['precision'] = str(precision)\n",
    "        evaluation_result['recall'] = str(recall)\n",
    "        evaluation_result['confusion_matrix'] = {}\n",
    "        evaluation_result['confusion_matrix']['tp'] = tp\n",
    "        evaluation_result['confusion_matrix']['fp'] = fp\n",
    "        evaluation_result['confusion_matrix']['tn'] = tn\n",
    "        evaluation_result['confusion_matrix']['fn'] = fn\n",
    "        \n",
    "        evaluation_data.append(evaluation_result)\n",
    "        print(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.759231', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866'}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.05}, 'training_and_loss_calc_time_in_sec': '0.713638', 'train_loss': '0.3987838539784226', 'test_loss': '0.39948206088489485', 'accuracy': '0.70718', 'f1_score': '0.6579764991706964', 'precision': '0.550611877859014', 'recall': '0.8173534532791642'}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 300, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '2.105392', 'train_loss': '0.3894785858650492', 'test_loss': '0.39012322881498535', 'accuracy': '0.71586', 'f1_score': '0.6649371477087804', 'precision': '0.5600492630408009', 'recall': '0.818165989553105'}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 300, 'gamma': 0.05}, 'training_and_loss_calc_time_in_sec': '2.093798', 'train_loss': '0.3905826615558456', 'test_loss': '0.3912308685795799', 'accuracy': '0.71422', 'f1_score': '0.6641280586700515', 'precision': '0.5580926796507724', 'recall': '0.8199071387115496'}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.764704', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866'}\n",
      "{'method': 'least_squares_SGD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '4.325422', 'train_loss': '1.782194603218081', 'test_loss': '1.7763050527157862', 'accuracy': '0.54988', 'f1_score': '0.48541247484909456', 'precision': '0.4004753640685128', 'recall': '0.6160766105629716'}\n",
      "{'method': 'ridge_regression', 'parameters': {'lambda': 100}, 'training_and_loss_calc_time_in_sec': '0.040133', 'train_loss': '0.4970710829640738', 'test_loss': '0.4970506235215197', 'accuracy': '0.63862', 'f1_score': '0.5369891095451634', 'precision': '0.48075246616196377', 'recall': '0.608125362739408'}\n",
      "{'method': 'least_squares', 'parameters': {}, 'training_and_loss_calc_time_in_sec': '0.023288', 'train_loss': '0.38885794352313174', 'test_loss': '0.38936695348103784', 'accuracy': '0.71676', 'f1_score': '0.6650425733207189', 'precision': '0.5612375249500998', 'recall': '0.8159605339524086'}\n",
      "{'method': 'logistic_regression', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '14.96455', 'train_loss': '-581240.0403015733', 'test_loss': '6886392392.59792', 'accuracy': '0.71078', 'f1_score': '0.6624337636266019', 'precision': '0.5540630247178726', 'recall': '0.8235055136390017'}\n",
      "{'method': 'logistic_regression', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '1.315524', 'train_loss': 'nan', 'test_loss': '6886392392.59792', 'accuracy': '0.71078', 'f1_score': '0.6624337636266019', 'precision': '0.5540630247178726', 'recall': '0.8235055136390017'}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.787177', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866', 'confusion_matrix': {'tp': 14125, 'fp': 11291, 'tn': 21479, 'fn': 3105}}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.780654', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866', 'confusion_matrix': {'tp': 14125, 'fp': 11291, 'tn': 21479, 'fn': 3105}}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.887559', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866', 'confusion_matrix': {'tp': 14125, 'fp': 11291, 'tn': 21479, 'fn': 3105}}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.809152', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866', 'confusion_matrix': {'tp': 14125, 'fp': 11291, 'tn': 21479, 'fn': 3105}}\n",
      "{'method': 'least_squares_GD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '0.787098', 'train_loss': '0.3925930461858477', 'test_loss': '0.3932601383256776', 'accuracy': '0.71208', 'f1_score': '0.6624302396473292', 'precision': '0.5557522820270696', 'recall': '0.8197910621009866', 'confusion_matrix': {'tp': 14125, 'fp': 11291, 'tn': 21479, 'fn': 3105}}\n",
      "{'method': 'least_squares_SGD', 'parameters': {'max_iter': 100, 'gamma': 0.1}, 'training_and_loss_calc_time_in_sec': '4.211122', 'train_loss': '77.66278943722936', 'test_loss': '77.87536604619282', 'accuracy': '0.50516', 'f1_score': '0.3945874522854067', 'precision': '0.341103308232507', 'recall': '0.46796285548461986', 'confusion_matrix': {'tp': 8063, 'fp': 15575, 'tn': 17195, 'fn': 9167}}\n",
      "{'method': 'least_squares', 'parameters': {}, 'training_and_loss_calc_time_in_sec': '0.024671', 'train_loss': '0.38885794352313174', 'test_loss': '0.38936695348103784', 'accuracy': '0.71676', 'f1_score': '0.6650425733207189', 'precision': '0.5612375249500998', 'recall': '0.8159605339524086', 'confusion_matrix': {'tp': 14059, 'fp': 10991, 'tn': 21779, 'fn': 3171}}\n",
      "{'method': 'ridge_regression', 'parameters': {'lambda': 100}, 'training_and_loss_calc_time_in_sec': '0.030448', 'train_loss': '0.4970710829640738', 'test_loss': '0.4970506235215197', 'accuracy': '0.63862', 'f1_score': '0.5369891095451634', 'precision': '0.48075246616196377', 'recall': '0.608125362739408', 'confusion_matrix': {'tp': 10478, 'fp': 11317, 'tn': 21453, 'fn': 6752}}\n"
     ]
    }
   ],
   "source": [
    "for el in evaluation_data:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

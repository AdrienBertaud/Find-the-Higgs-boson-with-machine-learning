{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "## Classification Using Linear Regression\n",
    "Load your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logistic_regression_unit_test_helpers import sample_data, load_data, standardize\n",
    "\n",
    "# load data.\n",
    "height, weight, gender = load_data()\n",
    "\n",
    "# build sampled x and y.\n",
    "seed = 1\n",
    "y = np.expand_dims(gender, axis=1)\n",
    "X = np.c_[height.reshape(-1), weight.reshape(-1)]\n",
    "y, X = sample_data(y, X, seed, size_samples=200)\n",
    "x, mean_x, std_x = standardize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute your cost by negative log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_ini(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_ini(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    pred = sigmoid_ini(tx.dot(w))\n",
    "    #print(\"pred = \", pred)\n",
    "    loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    #print(\"loss = \", -loss)\n",
    "    return np.squeeze(- loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient_ini(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    pred = sigmoid_ini(tx.dot(w))\n",
    "    grad = tx.T.dot(pred - y)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gradient Descent\n",
    "Implement your function to calculate the gradient for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent_ini(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss_ini(y, tx, w)\n",
    "    \n",
    "    grad = calculate_gradient_ini(y, tx, w)\n",
    "    w -= gamma * grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=138.62943611198904\n",
      "Current iteration=50, loss=47.04578583934811\n",
      "Current iteration=100, loss=43.46403230562902\n",
      "Current iteration=150, loss=42.1372011637542\n",
      "Current iteration=200, loss=41.545892808759405\n",
      "Current iteration=250, loss=41.253240090984214\n",
      "Current iteration=300, loss=41.098638973663114\n",
      "Current iteration=350, loss=41.013353756315546\n",
      "Current iteration=400, loss=40.96487063560558\n",
      "Current iteration=450, loss=40.93670847755999\n",
      "Current iteration=500, loss=40.92008945871305\n",
      "Current iteration=550, loss=40.9101659529988\n",
      "Current iteration=600, loss=40.90418744318991\n",
      "Current iteration=650, loss=40.90056108297143\n",
      "Current iteration=700, loss=40.89834994668996\n",
      "Current iteration=750, loss=40.89699628152846\n",
      "Current iteration=800, loss=40.8961649660955\n",
      "Current iteration=850, loss=40.895653191867964\n",
      "Current iteration=900, loss=40.89533753382109\n",
      "Current iteration=950, loss=40.89514254875611\n",
      "loss=40.895021964118996\n"
     ]
    }
   ],
   "source": [
    "from logistic_regression_unit_test_helpers import de_standardize\n",
    "\n",
    "def logistic_regression_gradient_descent_demo_ini(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 1000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.01\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent_ini(y, tx, w, gamma)\n",
    "        \n",
    "        # log info\n",
    "        if iter % 50 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "\n",
    "    print(\"loss={l}\".format(l=calculate_loss_ini(y, tx, w)))\n",
    "\n",
    "logistic_regression_gradient_descent_demo_ini(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration =  0 \tloss =  [[138.62943611]]\n",
      "Iteration =  50 \tloss =  [[47.04578584]]\n",
      "Iteration =  100 \tloss =  [[43.46403231]]\n",
      "Iteration =  150 \tloss =  [[42.13720116]]\n",
      "Iteration =  200 \tloss =  [[41.54589281]]\n",
      "Iteration =  250 \tloss =  [[41.25324009]]\n",
      "Iteration =  300 \tloss =  [[41.09863897]]\n",
      "Iteration =  350 \tloss =  [[41.01335376]]\n",
      "Iteration =  400 \tloss =  [[40.96487064]]\n",
      "Iteration =  450 \tloss =  [[40.93670848]]\n",
      "Iteration =  500 \tloss =  [[40.92008946]]\n",
      "Iteration =  550 \tloss =  [[40.91016595]]\n",
      "Iteration =  600 \tloss =  [[40.90418744]]\n",
      "Iteration =  650 \tloss =  [[40.90056108]]\n",
      "Iteration =  700 \tloss =  [[40.89834995]]\n",
      "Iteration =  750 \tloss =  [[40.89699628]]\n",
      "Iteration =  800 \tloss =  [[40.89616497]]\n",
      "Iteration =  850 \tloss =  [[40.89565319]]\n",
      "Iteration =  900 \tloss =  [[40.89533753]]\n",
      "Iteration =  950 \tloss =  [[40.89514255]]\n",
      "loss=[[40.89502196]]\n"
     ]
    }
   ],
   "source": [
    "from implementations import calculate_gradient_sigmoid, calculate_log_likelihood_loss, sigmoid, logistic_regression\n",
    "\n",
    "def logistic_regression_gradient_descent_demo_proj1(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 1000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.01\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    w, loss = logistic_regression(y, tx, w, max_iter, gamma)\n",
    "    \n",
    "    print(\"loss={l}\".format(l=calculate_log_likelihood_loss(y, tx, w)))\n",
    "\n",
    "logistic_regression_gradient_descent_demo_proj1(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using penalized logistic regression\n",
    "Fill in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression_ini(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss and gradient.\"\"\"\n",
    "    num_samples = y.shape[0]\n",
    "    loss = calculate_loss_ini(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "    gradient = calculate_gradient_ini(y, tx, w) + 2 * lambda_ * w\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient_ini(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, gradient = penalized_logistic_regression_ini(y, tx, w, lambda_)\n",
    "    w -= gamma * gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=138.62943611198904\n",
      "Current iteration=50, loss=48.637443262420284\n",
      "Current iteration=100, loss=46.076072692504965\n",
      "Current iteration=150, loss=45.36598486605834\n",
      "Current iteration=200, loss=45.136990805865345\n",
      "Current iteration=250, loss=45.05719234253381\n",
      "Current iteration=300, loss=45.02816989532987\n",
      "Current iteration=350, loss=45.01735027751011\n",
      "Current iteration=400, loss=45.013256927182994\n",
      "Current iteration=450, loss=45.01169445093317\n",
      "Current iteration=500, loss=45.011094778803134\n",
      "Current iteration=550, loss=45.01086385374369\n",
      "Current iteration=600, loss=45.01077474325605\n",
      "Current iteration=650, loss=45.01074031264927\n",
      "Current iteration=700, loss=45.01072699869884\n",
      "Current iteration=750, loss=45.01072184778359\n",
      "Current iteration=800, loss=45.01071985437807\n",
      "loss=41.80587008659321\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def logistic_regression_penalized_gradient_descent_demo_ini(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    gamma = 0.01\n",
    "    lambda_ = 0.1\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient_ini(y, tx, w, gamma, lambda_)\n",
    "        #print(w)\n",
    "        \n",
    "        # log info\n",
    "        if iter % 50 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "            \n",
    "    print(\"loss={l}\".format(l=calculate_loss_ini(y, tx, w)))\n",
    "    \n",
    "logistic_regression_penalized_gradient_descent_demo_ini(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration =  0 \tloss =  [[138.62943611]]\n",
      "Iteration =  50 \tloss =  [[48.63744326]]\n",
      "Iteration =  100 \tloss =  [[46.07607269]]\n",
      "Iteration =  150 \tloss =  [[45.36598487]]\n",
      "Iteration =  200 \tloss =  [[45.13699081]]\n",
      "Iteration =  250 \tloss =  [[45.05719234]]\n",
      "Iteration =  300 \tloss =  [[45.0281699]]\n",
      "Iteration =  350 \tloss =  [[45.01735028]]\n",
      "Iteration =  400 \tloss =  [[45.01325693]]\n",
      "Iteration =  450 \tloss =  [[45.01169445]]\n",
      "Iteration =  500 \tloss =  [[45.01109478]]\n",
      "Iteration =  550 \tloss =  [[45.01086385]]\n",
      "Iteration =  600 \tloss =  [[45.01077474]]\n",
      "Iteration =  650 \tloss =  [[45.01074031]]\n",
      "Iteration =  700 \tloss =  [[45.010727]]\n",
      "Iteration =  750 \tloss =  [[45.01072185]]\n",
      "Iteration =  800 \tloss =  [[45.01071985]]\n"
     ]
    }
   ],
   "source": [
    "from implementations import calculate_gradient_sigmoid, calculate_log_likelihood_loss, sigmoid, reg_logistic_regression\n",
    "\n",
    "def logistic_regression_penalized_gradient_descent_demo_proj1(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 801\n",
    "    gamma = 0.01\n",
    "    lambda_ = 0.1\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    w, loss = reg_logistic_regression(y, tx, lambda_, w, max_iter, gamma)\n",
    "    \n",
    "logistic_regression_penalized_gradient_descent_demo_proj1(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
